{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3: Generate Noisy Dataset (Final, Robust Version)\n",
    "\n",
    "This notebook downloads the ESC-50 dataset and uses it to create a `noisy_audio` directory by augmenting the files in the `clean_audio` directory.\n",
    "\n",
    "**Features:**\n",
    "- Downloads the dataset as a ZIP file to avoid `git` network errors.\n",
    "- Uses robust paths to correctly locate all necessary folders.\n",
    "- Displays progress bars for download and augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/zilongzeng/Research/Drone\n",
      "Dataset will be downloaded to: /Users/zilongzeng/Research/Drone/datasets/ESC-50.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. Setup & Path Definitions ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "zip_url = 'https://github.com/karolpiczak/ESC-50/archive/master.zip'\n",
    "datasets_dir = os.path.join(project_root, 'datasets')\n",
    "zip_path = os.path.join(datasets_dir, 'ESC-50.zip')\n",
    "extract_path = os.path.join(datasets_dir, 'ESC-50-raw')\n",
    "final_noise_dir = os.path.join(extract_path, 'ESC-50-master', 'audio')\n",
    "\n",
    "os.makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Dataset will be downloaded to: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ESC-50.zip from GitHub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596bd3012924477da5faf3d62006ed34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ESC-50: 0.00iB [00:00, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting /Users/zilongzeng/Research/Drone/datasets/ESC-50.zip...\n",
      "\n",
      "✅ Noise dataset successfully extracted to: /Users/zilongzeng/Research/Drone/datasets/ESC-50-raw\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Download and Extract ZIP ---\n",
    "print('Downloading ESC-50.zip from GitHub...')\n",
    "if os.path.exists(extract_path):\n",
    "    shutil.rmtree(extract_path)\n",
    "if os.path.exists(zip_path):\n",
    "    os.remove(zip_path)\n",
    "\n",
    "try:\n",
    "    response = requests.get(zip_url, stream=True)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "    with open(zip_path, 'wb') as f, tqdm(\n",
    "        desc='Downloading ESC-50',\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = f.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "    print(f'\\nExtracting {zip_path}...')\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "    os.remove(zip_path)\n",
    "    print(f'\\n✅ Noise dataset successfully extracted to: {os.path.abspath(extract_path)}')\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Download failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Clean audio directory not found: /Users/zilongzeng/Research/Drone/datasets/drone_data_for_training/clean_audio\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Data Augmentation Function ---\n",
    "def augment_with_noise(clean_audio_dir, noise_audio_dir, output_dir, snr_db=15, files_per_category=None):\n",
    "    if not os.path.isdir(clean_audio_dir):\n",
    "        print(f\"Error: Clean audio directory not found: {clean_audio_dir}\")\n",
    "        return\n",
    "        \n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    noise_files = [os.path.join(noise_audio_dir, f) for f in os.listdir(noise_audio_dir) if f.endswith('.wav')]\n",
    "    if not noise_files:\n",
    "        print(f\"Error: No noise files found in {noise_audio_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f'Starting augmentation with SNR = {snr_db} dB...')\n",
    "    \n",
    "    command_folders = [d for d in os.listdir(clean_audio_dir) if os.path.isdir(os.path.join(clean_audio_dir, d))]\n",
    "    for command in tqdm(command_folders, desc='Processing command folders'):\n",
    "        source_command_path = os.path.join(clean_audio_dir, command)\n",
    "        output_command_path = os.path.join(output_dir, command)\n",
    "        os.makedirs(output_command_path, exist_ok=True)\n",
    "        \n",
    "        files_to_process = [f for f in os.listdir(source_command_path) if f.endswith('.wav')]\n",
    "        if files_per_category and len(files_to_process) > files_per_category:\n",
    "            files_to_process = random.sample(files_to_process, files_per_category)\n",
    "            \n",
    "        for clean_file in files_to_process:\n",
    "            clean_path = os.path.join(source_command_path, clean_file)\n",
    "            speech, sr = librosa.load(clean_path, sr=16000)\n",
    "            \n",
    "            noise_path = random.choice(noise_files)\n",
    "            noise, _ = librosa.load(noise_path, sr=sr)\n",
    "            \n",
    "            if len(noise) < len(speech):\n",
    "                noise = np.pad(noise, (0, len(speech) - len(noise)), 'wrap')\n",
    "            \n",
    "            start = random.randint(0, len(noise) - len(speech))\n",
    "            noise_segment = noise[start:start + len(speech)]\n",
    "            \n",
    "            speech_power = np.mean(speech**2)\n",
    "            noise_power = np.mean(noise_segment**2)\n",
    "            snr = 10**(snr_db / 10)\n",
    "            scale = np.sqrt(speech_power / (snr * noise_power + 1e-10))\n",
    "            noisy_speech = speech + noise_segment * scale\n",
    "            \n",
    "            output_file_path = os.path.join(output_command_path, clean_file)\n",
    "            sf.write(output_file_path, noisy_speech, sr)\n",
    "    \n",
    "    print(f\"\\n✅ Augmentation complete. Noisy files saved in: {os.path.abspath(output_dir)}\")\n",
    "\n",
    "# --- Run Data Augmentation ---\n",
    "clean_data_dir = os.path.join(project_root, 'datasets', 'drone_data_for_training', 'clean_audio')\n",
    "noisy_output_dir = os.path.join(project_root, 'datasets', 'drone_data_for_training', 'noisy_audio')\n",
    "\n",
    "if os.path.exists(final_noise_dir):\n",
    "    augment_with_noise(clean_data_dir, final_noise_dir, noisy_output_dir)\n",
    "else:\n",
    "    print(f\"Error: Could not find the final noise directory at {final_noise_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
