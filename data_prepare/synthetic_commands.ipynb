{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2: Prepare Synthetic Speech Commands Dataset (Final, Robust Version)\n",
    "\n",
    "This notebook downloads the [Synthetic Speech Commands Dataset](https://www.kaggle.com/datasets/jbuchner/synthetic-speech-commands-dataset) and prepares it for training.\n",
    "\n",
    "**Features:**\n",
    "- **Skips Download**: Checks if the dataset directory already exists.\n",
    "- **Retry Logic**: Automatically retries the download up to 3 times on network failure.\n",
    "- **Progress Bars**: Displays progress for all major operations.\n",
    "- **Robust File Paths**: Ensures all paths are correct relative to the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import kaggle\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# --- 1. Setup & Path Definitions ---\n",
    "sample_rate = 16000\n",
    "duration = 1.0\n",
    "n_mels = 256\n",
    "desired_frames = 61\n",
    "\n",
    "movement_phrases = ['backward', 'forward', 'up', 'down', 'go', 'left', 'right', 'follow']\n",
    "emergency_phrases = ['stop', 'yes', 'no']\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "download_path = os.path.join(project_root, 'datasets', 'synthetic-speech-commands-raw')\n",
    "output_dir = os.path.join(project_root, 'processed_data', 'synthetic-speech-commands')\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Download Path: {download_path}\")\n",
    "print(f\"Output Path: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Download Dataset with Retry Logic ---\n",
    "dataset_slug = 'jbuchner/synthetic-speech-commands-dataset'\n",
    "max_retries = 3\n",
    "download_success = False\n",
    "\n",
    "if os.path.exists(download_path) and len(os.listdir(download_path)) > 1: # Check if folder is not empty\n",
    "    print('Dataset directory already exists. Skipping download.')\n",
    "    download_success = True\n",
    "else:\n",
    "    if os.path.exists(download_path):\n",
    "        shutil.rmtree(download_path)\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries}: Downloading dataset...\")\n",
    "            kaggle.api.dataset_download_files(dataset_slug, path=download_path, unzip=True, quiet=False)\n",
    "            download_success = True\n",
    "            print('\\n✅ Dataset downloaded and unzipped successfully.')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed on attempt {attempt + 1} with error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"Max retries reached. Download failed.\")\n",
    "\n",
    "if not download_success:\n",
    "    raise SystemExit(\"Fatal: Could not download the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Process Data ---\n",
    "if download_success:\n",
    "    x_data = []\n",
    "    y_labels = []\n",
    "    data_path = download_path\n",
    "\n",
    "    print(f'Processing audio files from {os.path.abspath(data_path)}...')\n",
    "    keyword_folders = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "    for keyword_folder in tqdm(keyword_folders, desc='Processing folders'):\n",
    "        if keyword_folder in emergency_phrases: label = 'emergency'\n",
    "        elif keyword_folder in movement_phrases: label = 'movement'\n",
    "        else: continue\n",
    "\n",
    "        keyword_path = os.path.join(data_path, keyword_folder)\n",
    "        for file in os.listdir(keyword_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                y_labels.append(label)\n",
    "                path = os.path.join(keyword_path, file)\n",
    "                audio, _ = librosa.load(path, sr=sample_rate, duration=duration)\n",
    "                if len(audio) < sample_rate: audio = np.pad(audio, (0, sample_rate - len(audio)))\n",
    "                else: audio = audio[:sample_rate]\n",
    "                \n",
    "                mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=n_mels)\n",
    "                mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "                mel_spec_db = mel_spec_db / 80.0 + 1.0\n",
    "                x_data.append(mel_spec_db)\n",
    "\n",
    "    print('\\n✅ Data processing complete.')\n",
    "\n",
    "    # --- 4. Format and Save Files ---\n",
    "    x_train = np.array(x_data)\n",
    "    x_train = x_train[..., np.newaxis]\n",
    "    current_frames = x_train.shape[2]\n",
    "    if current_frames < desired_frames:\n",
    "        pad_width = ((0, 0), (0, 0), (0, desired_frames - current_frames), (0, 0))\n",
    "        x_train = np.pad(x_train, pad_width, mode='constant')\n",
    "    elif current_frames > desired_frames:\n",
    "        x_train = x_train[:, :, :desired_frames, :]\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_doa_int = label_encoder.fit_transform(y_labels)\n",
    "    y_doa = to_categorical(y_doa_int)\n",
    "    y_sed = np.ones((len(y_doa), 1))\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(os.path.join(output_dir, 'x_train.npy'), x_train)\n",
    "    np.save(os.path.join(output_dir, 'y_doa.npy'), y_doa)\n",
    "    np.save(os.path.join(output_dir, 'y_sed.npy'), y_sed)\n",
    "    joblib.dump(label_encoder, os.path.join(output_dir, 'label_encoder.joblib'))\n",
    "\n",
    "    print(f\"\\n✅ Processed data saved to: {os.path.abspath(output_dir)}\")\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_doa shape: {y_doa.shape}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
